\section{Convergence of the Perceptron Algorithm}
\paragraph{Lemma 1 
\[
\vec{w}^{(k+1)}\cdot\vec{u}\geq k\gamma
\]
}


\paragraph*{Proof by induction}

Induction hypothesis:
\[
\vec{w}^{(k+1)}\cdot\vec{u}\geq k\gamma
\]
Base case: Show that the statement holds for $k=0$.
\[
\vec{w}^{(1)}\cdot\vec{u}=0\geq0\gamma
\]
Inductive step: Show that if the statement holds for $k$, it also
holds for $k+1$ by using the induction hypothesis and the linear
separability with finite margin. Let $i$ be the iteration step where
the \emph{k}\textsuperscript{th} mistake was made.
\begin{align*}
\vec{w}^{(k+1)}\cdot\vec{u} & =(\vec{w}^{(k)}+y_{i}\vec{x}_{i})\cdot\vec{u}=\vec{w}^{(k)}\cdot\vec{u}+y_{i}\vec{x}_{i}\cdot\vec{u}\\
 & \geq(k-1)\gamma+y_{i}\vec{x}_{i}\cdot\vec{u}\\
 & \geq(k-1)\gamma+\gamma\\
 & =k\gamma
\end{align*}
Thus, the statement holds for all natural numbers $k$.


\paragraph*{Lemma 2
\[
\left\Vert \vec{w}^{(k+1)}\right\Vert _{2}^{2}\leq\left\Vert \vec{w}^{(k)}\right\Vert _{2}^{2}+R^{2}
\]
}


\paragraph*{Proof}

Let $i$ be the iteration step where the \emph{k}\textsuperscript{th}
mistake was made.
\begin{align*}
\Vert\vec{w}^{(k+1)}\Vert_{2}^{2} & =\Vert\vec{w}^{(k)}+y_{i}\vec{x}_{i}\Vert_{2}^{2}\\
 & =\left(\vec{w}^{(k)}+y_{i}\vec{x}_{i}\right)\cdot\left(\vec{w}^{(k)}+y_{i}\vec{x}_{i}\right)\\
 & =\vec{w}^{(k)}\cdot\vec{w}^{(k)}+2y_{i}\vec{w}^{(k)}\cdot\vec{x}_{i}+y_{i}^{2}\vec{x}_{i}\cdot\vec{x}_{i}\\
 & =\Vert\vec{w}^{(k)}\Vert_{2}^{2}+2y_{i}\vec{w}^{(k)}\cdot\vec{x}_{i}+\Vert\vec{x}_{i}\Vert_{2}^{2}\\
 & \leq\Vert\vec{w}^{(k)}\Vert_{2}^{2}+\Vert\vec{x}_{i}\Vert_{2}^{2}\qquad\because y_{i}\vec{w}^{(k)}\cdot\vec{x}_{i}\leq0\\
 & \leq\Vert\vec{w}^{(k)}\Vert_{2}^{2}+R^{2}\qquad\because\Vert\vec{x}_{i}\Vert_{2}\leq R
\end{align*}



\paragraph*{Claim}

\[
\left|\left\{ i:1\le i\le n,\, y_{i}\,(\vec{w}_{i}\cdot\vec{x}_{i})\leq0\right\} \right|\leq\left(\frac{R}{\gamma}\right)^{2}
\]



\paragraph*{Proof}

The weights $\vec{w}_{k}$ are able to correctly classify any example
if
\[
\left\{ \vec{z}\in\mathbb{R}^{m}\mid\vec{z}\cdot\vec{w}_{k}=0\right\} \cap\left\{ \vec{z}\in\mathbb{R}^{m}\mid\Vert\vec{z}\Vert_{2}=0\right\} \subseteq\left\{ \vec{z}\in\mathbb{R}^{m}\mid\vert\vec{z}\cdot\vec{u}\vert<\gamma\right\} .
\]
\[
\Leftrightarrow\max_{\vec{z}\in\mathbb{R}^{m},\vec{z}\cdot\vec{w}_{k}=0,\Vert\vec{z}\Vert_{2}\leq R}\vec{z}\cdot\vec{u}=\sqrt{\left\Vert \frac{\vec{w}_{k}}{\Vert\vec{w}_{k}\Vert_{2}}R\right\Vert _{2}^{2}-\left(\vec{u}\cdot\frac{\vec{w}_{k}}{\Vert\vec{w}_{k}\Vert_{2}}R\right)^{2}}<\gamma.
\]
The Formula for the maximum of $\vec{z}\cdot\vec{u}$ can be derived
by geometrical reasoning or by evaluating the Karush\textendash{}Kuhn\textendash{}Tucker
(KKT) optimality conditions.

Show that the above condition is satisfied if $k>R^{2}/\gamma^{2}$.

\begin{align*}
k & >\frac{R^{2}}{\gamma^{2}}\\
\Leftrightarrow\gamma^{2} & >R^{2}-(k-1)\gamma^{2}\\
 & =R^{2}-\frac{\left(R\,\left(k-1\right)\gamma\right){}^{2}}{\left(k-1\right)R^{2}}\\
 & \geq R^{2}-\frac{R^{2}}{\Vert\vec{w}_{k}\Vert_{2}^{2}}\left(\vec{u}\cdot\vec{w}_{k}\right)^{2}\qquad\text{by lemma 1 and 2}\\
 & =\left\Vert \frac{\vec{w}_{k}}{\Vert\vec{w}_{k}\Vert_{2}}R\right\Vert ^{2}-\left(\vec{u}\cdot\frac{\vec{w}_{k}}{\Vert\vec{w}_{k}\Vert_{2}}R\right)^{2}\\
\Rightarrow\gamma & >\sqrt{\left\Vert \frac{\vec{w}_{k}}{\Vert\vec{w}_{k}\Vert_{2}}R\right\Vert ^{2}-\left(\vec{u}\cdot\frac{\vec{w}_{k}}{\Vert\vec{w}_{k}\Vert_{2}}R\right)^{2}}
\end{align*}
Therefore, if $k>R^{2}/\gamma^{2}$, no classification errors are
made. According to this bound, if $k\leq R^{2}/\gamma^{2}$, it is
possible that the algorithm makes a wrong classification. It follows
that 
\[
k_{\max}=\frac{R^{2}}{\gamma^{2}}+1
\]
 is the largest possible index. Since we start with no errors and
the index $k=1$, this means that the maximum number of wrong classifications
is $R^{2}/\gamma^{2}.$ Thus,

\[
\left|\left\{ i:1\le i\le n,\, y_{i}\,(\vec{w}_{i}\cdot\vec{x}_{i})\leq0\right\} \right|\leq\left(\frac{R}{\gamma}\right)^{2}.\quad\blacksquare
\]